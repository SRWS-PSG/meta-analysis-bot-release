"""
Parameter Collectorãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‹ã‚‰åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åé›†ã—ã€æ¤œè¨¼ã™ã‚‹æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""
import logging
import json
import time # è¿½åŠ 
from typing import Dict, Any, Optional, List, Tuple
import pandas as pd # pandas ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

from mcp.gemini_utils import extract_parameters_from_user_input
from mcp.dialog_state_manager import DialogStateManager # Assuming DialogStateManager is in its own file

logger = logging.getLogger(__name__)

class ParameterCollector:
    # Constants for parameter collection
    REQUIRED_PARAMS_DEFINITION = {
        "effect_size": None,  # OR, RR, HR, proportion, SMD, MD, COR, yi
        "model_type": None,   # fixed, random
    }
    OPTIONAL_PARAMS_DEFINITION = {
        "subgroup_columns": [], # List of strings
        "moderator_columns": [], # List of strings
        "sensitivity_variable": None,      # æ„Ÿåº¦åˆ†æã§ä½¿ç”¨ã™ã‚‹å¤‰æ•°å
        "sensitivity_value": None,         # é™å®šå¯¾è±¡ã¨ãªã‚‹å€¤
        # "ai_interpretation": True, # Default to True - Removed, will be hardcoded
        # "output_format": "detailed" # Default to detailed - Removed, will be hardcoded
    }
    EFFECT_SIZE_TO_ANALYSIS_TYPE_MAP = {
        "OR": "binary_outcome_two_groups", "RR": "binary_outcome_two_groups", "RD": "binary_outcome_two_groups",
        "HR": "hazard_ratio", "PETO": "binary_outcome_two_groups", # PETO is for binary
        "proportion": "single_proportion", "IR": "incidence_rate", # IR for incidence rates
        "SMD": "continuous_outcome_two_groups", "MD": "continuous_outcome_two_groups", "ROM": "continuous_outcome_two_groups",
        "COR": "correlation",
        "yi": "pre_calculated_effect_sizes" # For pre-calculated yi and vi
    }

    def __init__(self, context_manager, async_runner): # Add async_runner if needed for run_meta_analysis
        self.context_manager = context_manager
        self.async_runner = async_runner # Store async_runner

    def _update_collected_params_and_get_next_question(self, extracted_params_map: dict, collected_params_state: dict, data_summary: dict, thread_id: str, channel_id: str) -> tuple[bool, Optional[str]]:
        logger.info(f"Updating collected_params. Current: {collected_params_state}, Extracted: {extracted_params_map}")
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰gemini_questionsã‚’å–å¾—ï¼ˆæ­£ã—ã„ãƒ‘ã‚¹: data_state.gemini_analysis.suggested_questionsï¼‰
        context = self.context_manager.get_context(thread_id=thread_id, channel_id=channel_id)
        logger.info(f"DEBUG: parameter_collector - context from context_manager: {json.dumps(context, ensure_ascii=False)}") # DEBUG LOG
        gemini_analysis_state = context.get("data_state", {}).get("gemini_analysis", {})
        gemini_questions = gemini_analysis_state.get("suggested_questions", [])

        for param_key, value in extracted_params_map.items():
            if value is not None:
                if param_key in self.REQUIRED_PARAMS_DEFINITION:
                    collected_params_state["required"][param_key] = value
                    if param_key in collected_params_state["missing_required"]:
                        collected_params_state["missing_required"].remove(param_key)
                elif param_key in self.OPTIONAL_PARAMS_DEFINITION:
                    if isinstance(self.OPTIONAL_PARAMS_DEFINITION.get(param_key), list) and isinstance(value, list):
                         collected_params_state["optional"][param_key] = list(set(collected_params_state["optional"].get(param_key, []) + value))
                    else:
                        collected_params_state["optional"][param_key] = value
                elif param_key == "data_columns":
                    if isinstance(value, dict):
                        collected_params_state["optional"]["data_columns"] = value
        
        logger.info(f"Updated collected_params: {collected_params_state}")

        # Geminiã«ã‚ˆã‚‹è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°çµæœã‚’å–å¾—
        column_mappings_from_context = context.get("data_state", {}).get("column_mappings", {}) # Use already fetched context
        logger.info(f"DEBUG: parameter_collector - Column mappings from context: {json.dumps(column_mappings_from_context, ensure_ascii=False)}") # DEBUG LOG

        # è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿åˆ—ã‚’ collected_params_state["optional"]["data_columns"] ã«åæ˜ 
        if "data_columns" not in collected_params_state["optional"]:
            collected_params_state["optional"]["data_columns"] = {}
        
        target_mappings = column_mappings_from_context.get("target_role_mappings", {})
        if isinstance(target_mappings, dict): # target_mappingsãŒè¾æ›¸ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            for role_key, mapped_col_name in target_mappings.items():
                # ai, bi, ci, di, n1i, n2i, m1i, m2i, sd1i, sd2i, proportion_events, proportion_total, proportion_time, yi, vi
                # ãªã©ã€escalcã«å¿…è¦ãªåˆ—ã‚’å„ªå…ˆçš„ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                if role_key in self._get_all_escalc_roles() and not collected_params_state["optional"]["data_columns"].get(role_key) and mapped_col_name:
                    collected_params_state["optional"]["data_columns"][role_key] = mapped_col_name
                    logger.info(f"Auto-mapped data_column '{role_key}' to '{mapped_col_name}' from target_role_mappings")
        else:
            logger.warning(f"target_role_mappings is not a dict or is missing: {target_mappings}")


        # åŠ¹æœé‡ã®è‡ªå‹•æ¤œå‡ºã¨ç¢ºèª (required parametersãŒå®Œäº†ã—ã¦ã„ãªã„å ´åˆã§ã‚‚å®Ÿè¡Œ)
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã¾ã åŠ¹æœé‡ã‚’æŒ‡å®šã—ã¦ãŠã‚‰ãšã€ã‹ã¤ã€ã“ã®è³ªå•ãŒåˆå›ã§ãªã„ï¼ˆã¤ã¾ã‚Šã€ä»¥å‰ã«åŠ¹æœé‡ã‚’å°‹ã­ã¦ã„ãªã„ï¼‰å ´åˆã«ã®ã¿è‡ªå‹•æ¤œå‡ºã‚’è©¦ã¿ã‚‹
        # ãŸã ã—ã€GeminiãŒCSVåˆ†ææ™‚ã«åŠ¹æœé‡ã‚’æ¤œå‡ºã—ã¦ã„ãŸã‚‰ã€ãã‚Œã‚’å„ªå…ˆã™ã‚‹
        if not collected_params_state.get("required", {}).get("effect_size") and \
           ("effect_size" in collected_params_state.get("missing_required", []) or not collected_params_state.get("asked_optional")): # åˆå›è³ªå•ã¾ãŸã¯åŠ¹æœé‡ãŒæœªåé›†ã®å ´åˆ

            detected_effect_size = column_mappings_from_context.get("detected_effect_size")
            is_log_transformed = column_mappings_from_context.get("is_log_transformed")
            data_format = column_mappings_from_context.get("data_format")
            
            # detected_columns ã‹ã‚‰ yi ã¨ vi ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚‚ç¢ºèª (HRã®å ´åˆã«ç‰¹ã«é‡è¦)
            detected_columns_map = column_mappings_from_context.get("detected_columns", {})
            logger.info(f"DEBUG: parameter_collector - Initial detected_effect_size: {detected_effect_size}, is_log_transformed: {is_log_transformed}, data_format: {data_format}, detected_columns_map: {json.dumps(detected_columns_map, ensure_ascii=False)}") # DEBUG LOG
            
            if detected_effect_size:
                logger.info(f"DEBUG: parameter_collector - Auto-detected effect size: {detected_effect_size}, log_transformed: {is_log_transformed}, format: {data_format}, detected_cols: {json.dumps(detected_columns_map, ensure_ascii=False)}") # DEBUG LOG
                
                # è‡ªå‹•æ¤œå‡ºã•ã‚ŒãŸåŠ¹æœé‡ã‚’è¨­å®š
                collected_params_state["required"]["effect_size"] = detected_effect_size
                if "effect_size" in collected_params_state.get("missing_required", []):
                    collected_params_state["missing_required"].remove("effect_size")
                
                # å¯¾æ•°å¤‰æ›ã¨ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®æƒ…å ±ã‚’ä¿å­˜
                if is_log_transformed is not None:
                    collected_params_state["optional"]["is_log_transformed"] = is_log_transformed
                if data_format is not None:
                    collected_params_state["optional"]["data_format"] = data_format
                
                # HRã®å ´åˆã€yiã¨viã®ãƒãƒƒãƒ”ãƒ³ã‚°ãŒ target_role_mappings ã«ãªã‘ã‚Œã° detected_columns ã‹ã‚‰è£œå®Œ
                if detected_effect_size == "HR" and data_format == "pre_calculated":
                    if "yi" not in collected_params_state["optional"]["data_columns"] and detected_columns_map.get("yi"):
                        collected_params_state["optional"]["data_columns"]["yi"] = detected_columns_map["yi"]
                        logger.info(f"Auto-mapped data_column 'yi' to '{detected_columns_map['yi']}' from detected_columns for HR")
                    if "vi" not in collected_params_state["optional"]["data_columns"] and detected_columns_map.get("vi"):
                        collected_params_state["optional"]["data_columns"]["vi"] = detected_columns_map["vi"]
                        logger.info(f"Auto-mapped data_column 'vi' to '{detected_columns_map['vi']}' from detected_columns for HR")
                
                confirmation_parts = [f"ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŠ¹æœé‡ã€Œ{detected_effect_size}ã€ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚"]
                if is_log_transformed is True:
                    confirmation_parts.append("ãƒ‡ãƒ¼ã‚¿ã¯å¯¾æ•°å¤‰æ›æ¸ˆã¿ã¨ã—ã¦èªè­˜ã•ã‚Œã¾ã—ãŸã€‚")
                elif is_log_transformed is False:
                    confirmation_parts.append("ãƒ‡ãƒ¼ã‚¿ã¯å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã¨ã—ã¦èªè­˜ã•ã‚Œã¾ã—ãŸã€‚")
                
                if data_format == "2x2_table":
                    confirmation_parts.append("2Ã—2è¡¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†ã•ã‚Œã¾ã™ã€‚")
                elif data_format == "pre_calculated":
                    confirmation_parts.append("äº‹å‰è¨ˆç®—ã•ã‚ŒãŸåŠ¹æœé‡ã¨ã—ã¦å‡¦ç†ã•ã‚Œã¾ã™ã€‚")
                    # äº‹å‰è¨ˆç®—ã®å ´åˆã€yiã¨viãŒãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                    current_yi = collected_params_state["optional"]["data_columns"].get("yi")
                    current_vi = collected_params_state["optional"]["data_columns"].get("vi")
                    if current_yi and current_vi:
                        confirmation_parts.append(f"(åŠ¹æœé‡: {current_yi}, åˆ†æ•£/SE: {current_vi})")
                    else:
                        confirmation_parts.append("(åŠ¹æœé‡ã¾ãŸã¯åˆ†æ•£/SEã®åˆ—ãŒã¾ã ç‰¹å®šã§ãã¦ã„ã¾ã›ã‚“)")

                confirmation_parts.append("ã“ã®åŠ¹æœé‡ã§åˆ†æã‚’é€²ã‚ã¦ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿ (ã¯ã„/ã„ã„ãˆã€ã¾ãŸã¯åˆ¥ã®åŠ¹æœé‡ã‚’æŒ‡å®š)")
                # ã“ã®è³ªå•ã‚’ã€Œæœ€å¾Œã«å°‹ã­ãŸè³ªå•ã€ã¨ã—ã¦è¨˜éŒ²ã—ãªã„ã‚ˆã†ã«ã™ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç”±ãªå›ç­”ã‚’æœŸå¾…ã™ã‚‹ãŸã‚ï¼‰
                # context["question_history"]["last_question"] = "\n".join(confirmation_parts) # æ›´æ–°ã—ãªã„
                return False, "\n".join(confirmation_parts)

        if not collected_params_state["missing_required"]:
            effect_size = collected_params_state.get("required", {}).get("effect_size")
            data_columns = collected_params_state.get("optional", {}).get("data_columns", {})
            
            missing_data_cols_question = self._get_missing_data_columns_question(effect_size, data_columns, data_summary.get('columns', []), target_mappings) # column_mappings ã‚’ target_mappings ã«å¤‰æ›´
            if missing_data_cols_question:
                return False, missing_data_cols_question

            # ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—åˆ—ã®è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°ã¨è³ªå•ã®èª¿æ•´
            if not collected_params_state["optional"].get("subgroup_columns") and "subgroup_columns" not in collected_params_state["asked_optional"]:
                subgroup_candidates = column_mappings_from_context.get("suggested_subgroup_candidates", [])
                
                # gemini_questionsã‹ã‚‰ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—è§£æç”¨ã®å…·ä½“ä¾‹ã‚’å–å¾—
                subgroup_examples = []
                for question in gemini_questions:
                    if "ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—" in question.get("purpose", "") and question.get("variable_names"):
                        subgroup_examples = question.get("variable_names", [])
                        break
                
                if subgroup_candidates and isinstance(subgroup_candidates, list):
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«æŒ‡å®šã—ã¦ã„ãªã„å ´åˆã®ã¿ã€å€™è£œã‚’åˆæœŸå€¤ã¨ã—ã¦è¨­å®š
                    if not collected_params_state["optional"].get("subgroup_columns"):
                        collected_params_state["optional"]["subgroup_columns"] = subgroup_candidates
                        logger.info(f"Auto-set subgroup_columns to candidates: {subgroup_candidates}")
                    
                    # å€™è£œã‚’æç¤ºã™ã‚‹è³ªå•æ–‡
                    if subgroup_examples:
                        question_text = f"ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—è§£æã‚’è¡Œã†å ´åˆã€ã©ã®å¤‰æ•°ï¼ˆä¾‹ï¼š{', '.join(subgroup_examples)}ï¼‰ã§å±¤åˆ¥åŒ–ã—ã¾ã™ã‹ï¼Ÿ\nå€™è£œã¨ãªã‚‹åˆ—: {', '.join(subgroup_candidates)}"
                    else:
                        question_text = f"ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—è§£æã¯è¡Œã„ã¾ã™ã‹ï¼Ÿå€™è£œã¨ãªã‚‹åˆ—: {', '.join(subgroup_candidates)}ã€‚\nã‚‚ã—è¡Œã†å ´åˆã¯ã€ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç¤ºã™åˆ—åï¼ˆè¤‡æ•°å¯ï¼‰ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚ï¼ˆä¾‹: subgroup_columns: [age_group, gender]ï¼‰"
                else:
                    if subgroup_examples:
                        question_text = f"ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—è§£æã‚’è¡Œã†å ´åˆã€ã©ã®å¤‰æ•°ï¼ˆä¾‹ï¼š{', '.join(subgroup_examples)}ï¼‰ã§å±¤åˆ¥åŒ–ã—ã¾ã™ã‹ï¼Ÿ"
                    else:
                        question_text = "ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—è§£æã¯è¡Œã„ã¾ã™ã‹ï¼Ÿã‚‚ã—è¡Œã†å ´åˆã¯ã€ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç¤ºã™åˆ—åï¼ˆè¤‡æ•°å¯ï¼‰ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚ï¼ˆä¾‹: subgroup_columns: [age_group, gender]ï¼‰"
                
                collected_params_state["asked_optional"].append("subgroup_columns")
                return False, question_text

            # ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚¿ãƒ¼åˆ—ã®è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°ã¨è³ªå•ã®èª¿æ•´
            if not collected_params_state["optional"].get("moderator_columns") and "moderator_columns" not in collected_params_state["asked_optional"]:
                 moderator_candidates = column_mappings_from_context.get("suggested_moderator_candidates", [])
                 
                 # gemini_questionsã‹ã‚‰ãƒ¡ã‚¿å›å¸°åˆ†æç”¨ã®å…·ä½“ä¾‹ã‚’å–å¾—
                 moderator_examples = []
                 for question in gemini_questions:
                     if "ãƒ¡ã‚¿å›å¸°" in question.get("purpose", "") or "ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚¿ãƒ¼" in question.get("purpose", ""):
                         moderator_examples = question.get("variable_names", [])
                         break
                 
                 if moderator_candidates and isinstance(moderator_candidates, list):
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«æŒ‡å®šã—ã¦ã„ãªã„å ´åˆã®ã¿ã€å€™è£œã‚’åˆæœŸå€¤ã¨ã—ã¦è¨­å®š
                    if not collected_params_state["optional"].get("moderator_columns"):
                        collected_params_state["optional"]["moderator_columns"] = moderator_candidates
                        logger.info(f"Auto-set moderator_columns to candidates: {moderator_candidates}")

                    # å€™è£œã‚’æç¤ºã™ã‚‹è³ªå•æ–‡
                    if moderator_examples:
                        question_text = f"ãƒ¡ã‚¿å›å¸°åˆ†æã‚’è¡Œã†å ´åˆã€ã©ã®å¤‰æ•°ï¼ˆä¾‹ï¼š{', '.join(moderator_examples)}ï¼‰ã‚’å…±å¤‰é‡ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ\nå€™è£œã¨ãªã‚‹åˆ—: {', '.join(moderator_candidates)}"
                    else:
                        question_text = f"ãƒ¡ã‚¿å›å¸°åˆ†æã¯è¡Œã„ã¾ã™ã‹ï¼Ÿå€™è£œã¨ãªã‚‹åˆ—: {', '.join(moderator_candidates)}ã€‚\nã‚‚ã—è¡Œã†å ´åˆã¯ã€å…±å¤‰é‡ï¼ˆãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼‰ã¨ãªã‚‹åˆ—åã‚’æ•™ãˆã¦ãã ã•ã„ã€‚ï¼ˆä¾‹: moderator_columns: [year, dosage]ï¼‰"
                 else:
                    if moderator_examples:
                        question_text = f"ãƒ¡ã‚¿å›å¸°åˆ†æã‚’è¡Œã†å ´åˆã€ã©ã®å¤‰æ•°ï¼ˆä¾‹ï¼š{', '.join(moderator_examples)}ï¼‰ã‚’å…±å¤‰é‡ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ"
                    else:
                        question_text = "ãƒ¡ã‚¿å›å¸°åˆ†æã¯è¡Œã„ã¾ã™ã‹ï¼Ÿã‚‚ã—è¡Œã†å ´åˆã¯ã€å…±å¤‰é‡ï¼ˆãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼‰ã¨ãªã‚‹åˆ—åã‚’æ•™ãˆã¦ãã ã•ã„ã€‚ï¼ˆä¾‹: moderator_columns: [year, dosage]ï¼‰"
                 
                 collected_params_state["asked_optional"].append("moderator_columns")
                 return False, question_text

            # æ„Ÿåº¦åˆ†æã®è³ªå•
            if not collected_params_state["optional"].get("sensitivity_variable") and "sensitivity_variable" not in collected_params_state["asked_optional"]:
                sensitivity_candidates_formatted = []
                note_for_n1_filter = "(n=1ã®ã‚«ãƒ†ã‚´ãƒªã¯ãƒ¡ã‚¿ã‚¢ãƒŠãƒªã‚·ã‚¹ãŒå®Ÿæ–½ã§ããªã„ãŸã‚ã€è¡¨ç¤ºã‹ã‚‰é™¤å¤–ã—ã¦ã„ã¾ã™)"
                
                if data_summary and data_summary.get("columns"):
                    # data_summary['head'] ã¯ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­éƒ¨åˆ†ã®ã¿ã§ã™ã€‚
                    # æ­£ç¢ºãªã‚«ãƒ†ã‚´ãƒªã®å‡ºç¾å›æ•°ã‚’å¾—ã‚‹ã«ã¯ã€å…¨ãƒ‡ãƒ¼ã‚¿ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒå¿…è¦ã§ã™ã€‚
                    # ã“ã“ã§ã¯ df_sample_head ã‚’ä½¿ã„ã¾ã™ãŒã€ã“ã‚Œã¯è¿‘ä¼¼çš„ãªæƒ…å ±ã¨ãªã‚Šã¾ã™ã€‚
                    df_sample_head = pd.DataFrame(data_summary.get("head", [])) 
                    if not df_sample_head.empty:
                        for col in data_summary.get("columns", []):
                            if col in df_sample_head.columns and \
                               (df_sample_head[col].dtype == 'object' or pd.api.types.is_string_dtype(df_sample_head[col])):
                                
                                # headãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã¨ãã®å‡ºç¾å›æ•°ã‚’å–å¾—
                                value_counts_in_head = df_sample_head[col].value_counts()
                                # å‡ºç¾å›æ•°ãŒ2å›ä»¥ä¸Šã®ã‚«ãƒ†ã‚´ãƒªã®ã¿ã‚’æŠ½å‡º (headã®ç¯„å›²å†…)
                                valid_categories_in_head = [
                                    str(idx) for idx, count in value_counts_in_head.items() if count > 1
                                ]
                                
                                # å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯: ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚«ãƒ†ã‚´ãƒªæ•°ãŒ1ã‚ˆã‚Šå¤šã5ä»¥ä¸‹ã®å ´åˆã«å€™è£œã¨ã™ã‚‹
                                # ã“ã“ã§ã¯ã€n>1ã®æœ‰åŠ¹ãªã‚«ãƒ†ã‚´ãƒªãŒæŠ½å‡ºã§ããŸå ´åˆã«å€™è£œã¨ã™ã‚‹
                                # ã•ã‚‰ã«ã€ãã®æœ‰åŠ¹ãªã‚«ãƒ†ã‚´ãƒªã®æ•°ãŒ1ã‚ˆã‚Šå¤šã5ä»¥ä¸‹ã§ã‚ã‚‹ã‹ã‚‚ãƒã‚§ãƒƒã‚¯ã™ã‚‹
                                if valid_categories_in_head and (1 < len(valid_categories_in_head) <= 5):
                                    sensitivity_candidates_formatted.append(f"{col} ({', '.join(valid_categories_in_head)})")
                
                if sensitivity_candidates_formatted:
                    question_text = (
                        f"æ„Ÿåº¦åˆ†æï¼ˆç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªã«é™å®šã—ãŸåˆ†æï¼‰ã‚’è¡Œã„ã¾ã™ã‹ï¼Ÿ\n"
                        f"å€™è£œã¨ãªã‚‹å¤‰æ•°ã¨ã‚«ãƒ†ã‚´ãƒª (ãƒ‡ãƒ¼ã‚¿å…ˆé ­éƒ¨åˆ†ã«åŸºã¥ã):\n"
                        f"{'; '.join(sensitivity_candidates_formatted)}\n"
                        f"{note_for_n1_filter}"
                    )
                else:
                    question_text = (
                        "æ„Ÿåº¦åˆ†æï¼ˆç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªã«é™å®šã—ãŸåˆ†æï¼‰ã‚’è¡Œã„ã¾ã™ã‹ï¼Ÿ\n"
                        "(é©åˆ‡ãªå€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿å…ˆé ­éƒ¨åˆ†ã«è¤‡æ•°å›å‡ºç¾ã—ã€ã‚«ãƒ†ã‚´ãƒªæ•°ãŒ2ï½5å€‹ã®å¤‰æ•°ãŒå°‘ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™)\n"
                        f"{note_for_n1_filter}"
                    )
                collected_params_state["asked_optional"].append("sensitivity_variable")
                return False, question_text

            if collected_params_state["optional"].get("sensitivity_variable") and \
               not collected_params_state["optional"].get("sensitivity_value") and \
               "sensitivity_value" not in collected_params_state["asked_optional"]:
                
                sensitivity_var = collected_params_state["optional"]["sensitivity_variable"]
                # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªå€¤ã‚’å–å¾— (data_summaryã®headã‹ã‚‰)
                # CsvProcessorãŒå…¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ã‚’æ¸¡ã™æ–¹ãŒã‚ˆã‚Šæ­£ç¢º
                # ã“ã“ã§ã¯ data_summary.head ã‚’ä½¿ã†ç°¡æ˜“çš„ãªæ–¹æ³•
                unique_values_for_var = []
                if data_summary and data_summary.get("head"):
                    df_sample_head = pd.DataFrame(data_summary.get("head", []))
                    if sensitivity_var in df_sample_head.columns:
                        unique_values_for_var = df_sample_head[sensitivity_var].unique().tolist()
                
                if unique_values_for_var:
                    question_text = f"å¤‰æ•°ã€Œ{sensitivity_var}ã€ã®ã©ã®å€¤ã«é™å®šã—ãŸåˆ†æã‚’è¡Œã„ã¾ã™ã‹ï¼Ÿ\né¸æŠè‚¢: {', '.join(map(str, unique_values_for_var))}"
                else:
                    question_text = f"å¤‰æ•°ã€Œ{sensitivity_var}ã€ã«é™å®šã™ã‚‹å€¤ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
                
                collected_params_state["asked_optional"].append("sensitivity_value")
                return False, question_text
            
            logger.info("All required, data_columns, and optional (or asked) parameters collected. Ready for analysis.")
            return True, None

        next_missing_required = collected_params_state["missing_required"][0]
        questions_map = {
            "effect_size": f"ã©ã®åŠ¹æœé‡ï¼ˆä¾‹ï¼šOR, RR, SMD, HR, proportion, yiï¼‰ã§åˆ†æã—ã¾ã™ã‹ï¼Ÿ\nåˆ©ç”¨å¯èƒ½ãªåˆ—: {', '.join(data_summary.get('columns', []))}",
            "model_type": "å›ºå®šåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆfixedï¼‰ã¾ãŸã¯ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆrandomï¼‰ã®ã©ã¡ã‚‰ã‚’ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ"
        }
        logger.info(f"Missing required parameter: {next_missing_required}. Asking question.")
        return False, questions_map.get(next_missing_required, "è¿½åŠ æƒ…å ±ãŒå¿…è¦ã§ã™ã€‚")

    def _get_all_escalc_roles(self) -> List[str]:
        """escalcã§ä½¿ç”¨å¯èƒ½ãªå…¨ã¦ã®åˆ—ãƒ­ãƒ¼ãƒ«ã‚’è¿”ã™"""
        return ["ai", "bi", "ci", "di", "n1i", "n2i", "m1i", "m2i", "sd1i", "sd2i", 
                "proportion_events", "proportion_total", "proportion_time", "yi", "vi",
                "study_label", "study_label_author", "study_label_year"]

    def _get_missing_data_columns_question(self, effect_size: Optional[str], current_data_cols: Dict[str, str], available_csv_columns: List[str], column_mappings: Dict[str, Any]) -> Optional[str]:
        if not effect_size:
            return None

        required_mapping = {}
        col_descriptions = {
            "study_label_author": "ç ”ç©¶ã®è‘—è€…åã‚’ç¤ºã™åˆ—", "study_label_year": "ç ”ç©¶ã®ç™ºè¡¨å¹´ã‚’ç¤ºã™åˆ—", "study_label": "ç ”ç©¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ©ãƒ™ãƒ«ã‚’ç¤ºã™åˆ—",
            "ai": "æ²»ç™‚ç¾¤ã®ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚’ç¤ºã™åˆ—", "bi": "æ²»ç™‚ç¾¤ã®éã‚¤ãƒ™ãƒ³ãƒˆæ•°ï¼ˆã¾ãŸã¯æ²»ç™‚ç¾¤ã®ç·æ•°ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚’å¼•ã„ãŸã‚‚ã®ï¼‰ã‚’ç¤ºã™åˆ—",
            "ci": "å¯¾ç…§ç¾¤ã®ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚’ç¤ºã™åˆ—", "di": "å¯¾ç…§ç¾¤ã®éã‚¤ãƒ™ãƒ³ãƒˆæ•°ï¼ˆã¾ãŸã¯å¯¾ç…§ç¾¤ã®ç·æ•°ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚’å¼•ã„ãŸã‚‚ã®ï¼‰ã‚’ç¤ºã™åˆ—",
            "n1i": "æ²»ç™‚ç¾¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’ç¤ºã™åˆ—", "n2i": "å¯¾ç…§ç¾¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’ç¤ºã™åˆ—",
            "m1i": "æ²»ç™‚ç¾¤ã®å¹³å‡å€¤ã‚’ç¤ºã™åˆ—", "m2i": "å¯¾ç…§ç¾¤ã®å¹³å‡å€¤ã‚’ç¤ºã™åˆ—",
            "sd1i": "æ²»ç™‚ç¾¤ã®æ¨™æº–åå·®ã‚’ç¤ºã™åˆ—", "sd2i": "å¯¾ç…§ç¾¤ã®æ¨™æº–åå·®ã‚’ç¤ºã™åˆ—",
            "proportion_events": "å‰²åˆè¨ˆç®—ã®ãŸã‚ã®ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚’ç¤ºã™åˆ—", "proportion_total": "å‰²åˆè¨ˆç®—ã®ãŸã‚ã®ç·æ•°ã‚’ç¤ºã™åˆ—",
            "yi": "äº‹å‰è¨ˆç®—ã•ã‚ŒãŸåŠ¹æœé‡ã‚’ç¤ºã™åˆ—", "vi": "äº‹å‰è¨ˆç®—ã•ã‚ŒãŸåŠ¹æœé‡ã®åˆ†æ•£ã‚’ç¤ºã™åˆ—",
        }

        if effect_size in ["OR", "RR", "RD", "PETO"]: 
            # bi, di ã¯ n1i/n2i ã¨ ai/ci ã‹ã‚‰è¨ˆç®—å¯èƒ½ãªã®ã§ã€ã“ã“ã§ã¯å¿…é ˆã¨ã—ãªã„
            # å¿…è¦ãªã®ã¯ ai, ci ã¨ã€(bi, di) ã¾ãŸã¯ (n1i, n2i) ã®çµ„ã¿åˆã‚ã›
            required_mapping = {"ai": None, "ci": None} 
            # n1i, n2i ã®ãƒã‚§ãƒƒã‚¯ã¯ã€bi, di ãŒãªã„å ´åˆã« _generate_escalc_code ã§è¡Œã‚ã‚Œã‚‹
            # ã“ã“ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å°‹ã­ã‚‹ã¹ãã¯ã€ai, ci, ãã—ã¦ bi/di ãŒãªã‘ã‚Œã° n1i/n2i
            # ã‚ˆã‚Šå…·ä½“çš„ã«ä¸è¶³ã‚’æŒ‡æ‘˜ã™ã‚‹ãŸã‚ã«ã€ã“ã“ã§ã¯ã¾ãš ai, ci ã‚’ç¢ºèª
            # bi, di ãŒãªã‘ã‚Œã°ã€æ¬¡ã« n1i, n2i ã‚’å°‹ã­ã‚‹ã¨ã„ã†å¤šæ®µéšã®è³ªå•ã‚‚è€ƒãˆã‚‰ã‚Œã‚‹ãŒã€
            # Geminiã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡ºã¨Rã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆå´ã§ã‚«ãƒãƒ¼ã§ãã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã€
            # ã“ã“ã§ã¯åŸºæœ¬çš„ãª ai, ci ã®ã¿å¿…é ˆã¨ã™ã‚‹ã€‚
            # ã‚‚ã— bi, di ãŒãªãã€n1i, n2i ã‚‚ãªã„å ´åˆã¯ã€Rã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆæ™‚ã«ã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹ã€‚
        elif effect_size in ["SMD", "MD", "ROM"]: required_mapping = {"n1i": None, "n2i": None, "m1i": None, "m2i": None, "sd1i": None, "sd2i": None}
        elif effect_size == "proportion": required_mapping = {"proportion_events": None, "proportion_total": None}
        elif effect_size == "IR": required_mapping = {"proportion_events": None, "proportion_time": "ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”ŸæœŸé–“ã¾ãŸã¯è¿½è·¡æ™‚é–“ã‚’ç¤ºã™åˆ—"}
        elif effect_size == "yi": required_mapping = {"yi": None, "vi": None}
        
        missing_cols = [key for key in required_mapping if key not in current_data_cols or not current_data_cols[key]]
        if not missing_cols: return None

        next_missing_col_key = missing_cols[0]
        description = col_descriptions.get(next_missing_col_key, next_missing_col_key)
        
        return (
            f"åŠ¹æœé‡ã€Œ{effect_size}ã€ã®åˆ†æã«ã¯ã€{description}ãŒå¿…è¦ã§ã™ã€‚\n"
            f"CSVãƒ•ã‚¡ã‚¤ãƒ«å†…ã®åˆ©ç”¨å¯èƒ½ãªåˆ—ã¯æ¬¡ã®é€šã‚Šã§ã™: {', '.join(available_csv_columns)}\n"
            f"ã€Œ{description}ã€ã«å¯¾å¿œã™ã‚‹åˆ—åã‚’æ•™ãˆã¦ãã ã•ã„ã€‚ï¼ˆä¾‹: {next_missing_col_key}: actual_column_nameï¼‰"
        )

    def handle_analysis_preference_dialog(self, text: str, thread_ts: str, channel_id: str, client, context: dict, run_meta_analysis_func, check_analysis_job_func):
        logger.info(f"=== handle_analysis_preference_dialog (State: {context.get('dialog_state', {}).get('state')}) ===")
        logger.info(f"Received text for parameter collection in handle_analysis_preference_dialog: '{text}'")

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†: 5åˆ†ä»¥ä¸ŠçµŒéã—ã¦ã„ãŸã‚‰å¼·åˆ¶çš„ã«ãƒ•ãƒ©ã‚°ã‚’ã‚¯ãƒªã‚¢
        if context.get("parameter_collection_in_progress"):
            started_at = context.get("parameter_collection_started_at", 0)
            if time.time() - started_at > 300:  # 5åˆ†
                logger.warning(f"Parameter collection for thread {thread_ts} timed out after 5 minutes. Resetting flag.")
                context["parameter_collection_in_progress"] = False
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸã“ã¨ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é€šçŸ¥ã—ã¦ã‚‚è‰¯ã„ã‹ã‚‚ã—ã‚Œãªã„

        # # å‡¦ç†ä¸­ãƒ•ãƒ©ã‚°ã®ãƒã‚§ãƒƒã‚¯ (ã‚´ãƒ¼ã‚¹ãƒˆãƒ¡ãƒ³ã‚·ãƒ§ãƒ³å¯¾ç­–ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ)
        # if context.get("parameter_collection_in_progress"):
        #     # çµŒéæ™‚é–“ã«å¿œã˜ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºã—åˆ†ã‘ã‚‹
        #     started_at = context.get("parameter_collection_started_at", time.time())
        #     elapsed_time = time.time() - started_at
            
        #     if elapsed_time < 60: # 1åˆ†æœªæº€
        #         wait_message = "ç¾åœ¨ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è§£æä¸­ã§ã™ã€‚å°‘ã€…ãŠå¾…ã¡ãã ã•ã„... ğŸ¤”"
        #     elif elapsed_time < 180: # 3åˆ†æœªæº€
        #         wait_message = "ã¾ã è€ƒãˆä¸­ã§ã™ã€‚ã‚‚ã†å°‘ã—ãŠå¾…ã¡ãã ã•ã„... ğŸ’­"
        #     else: # 3åˆ†ä»¥ä¸Š
        #         wait_message = "è§£æå‡¦ç†ãŒé€²è¡Œä¸­ã§ã™ã€‚å®Œäº†ã¾ã§ä»Šã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„... â³"
            
        #     try:
        #         client.chat_postMessage(
        #             channel=channel_id,
        #             thread_ts=thread_ts,
        #             text=wait_message
        #         )
        #     except Exception as e:
        #         logger.error(f"Failed to post 'parameter collection in progress' message: {e}")
        #     self.context_manager.save_context(thread_ts, context, channel_id) # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
        #     return

        data_state = context.get("data_state", {})
        data_summary = data_state.get("summary", {})
        dialog_state = context.get("dialog_state", {})
        collected_params_state = dialog_state.get("collected_params")

        if "question_history" not in context:
            context["question_history"] = {"last_question": None, "count": 0, "max_retries": 5}
            logger.info(f"Initialized question_history for thread {thread_ts}")
        elif "max_retries" not in context["question_history"]:
            context["question_history"]["max_retries"] = 5

        if not data_summary:
            logger.error("Data summary not found. Cannot proceed.")
            client.chat_postMessage(channel=channel_id, thread_ts=thread_ts, text="ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€åˆ†æè¨­å®šã‚’é€²ã‚ã‚‰ã‚Œã¾ã›ã‚“ã€‚")
            self.context_manager.save_context(thread_ts, context, channel_id)
            return

        if dialog_state.get("state") == "collecting_params" and collected_params_state:
            thinking_message_ts = None
            try:
                # å‡¦ç†é–‹å§‹
                context["parameter_collection_in_progress"] = True
                context["parameter_collection_started_at"] = time.time()
                self.context_manager.save_context(thread_ts, context, channel_id) # ãƒ•ãƒ©ã‚°è¨­å®šå¾Œã™ãã«ä¿å­˜

                # text ãŒã‚ã‚‹å ´åˆã®ã¿ã€Œæ¤œè¨ä¸­ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºã™ (Geminiå‘¼ã³å‡ºã—ãŒã‚ã‚‹ãŸã‚)
                if text:
                    response = client.chat_postMessage(channel=channel_id, thread_ts=thread_ts, text="è§£ææ–¹æ³•ã‚’æ¤œè¨ä¸­ã§ã™ã€‚å°‘ã€…ãŠå¾…ã¡ãã ã•ã„...")
                    thinking_message_ts = response.get("ts")
            except Exception as e: 
                logger.error(f"Failed to post 'thinking' message or set progress flag: {e}")
                # ãƒ•ãƒ©ã‚°ãŒè¨­å®šã•ã‚ŒãŸã¾ã¾ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯Falseã«æˆ»ã™è©¦ã¿
                context["parameter_collection_in_progress"] = False
                self.context_manager.save_context(thread_ts, context, channel_id)


            try:
                extracted_params_map = {}
                if text:
                    # ä¼šè©±å±¥æ­´ã¨åé›†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
                    raw_history = context.get("history", [])
                    conversation_history_for_gemini = []
                    for entry in raw_history:
                        if "user" in entry and entry["user"]:
                            conversation_history_for_gemini.append({"role": "user", "content": entry["user"]})
                        if "bot" in entry and entry["bot"]:
                             conversation_history_for_gemini.append({"role": "model", "content": entry["bot"]}) # Geminiã¯ 'model' ã‚’æœŸå¾…

                    # ç¾åœ¨ã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦è³ªå•ã—ã¦ã„ã‚‹ã‹ã‚’ç‰¹å®šã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
                    current_question_target = None
                    if collected_params_state.get("missing_required"):
                        current_question_target = collected_params_state["missing_required"][0]
                    elif "subgroup_columns" not in collected_params_state.get("asked_optional", []):
                        current_question_target = "subgroup_columns"
                    elif "moderator_columns" not in collected_params_state.get("asked_optional", []):
                        current_question_target = "moderator_columns"
                    elif not self._get_missing_data_columns_question(collected_params_state.get("required", {}).get("effect_size"), collected_params_state.get("optional", {}).get("data_columns", {}), data_summary.get('columns', []), context.get("data_state", {}).get("column_mappings", {}).get("target_role_mappings", {})):
                         current_question_target = "data_columns"


                    collection_context_for_gemini = {
                        "phase": "parameter_collection",
                        "purpose": "ãƒ¡ã‚¿ã‚¢ãƒŠãƒªã‚·ã‚¹ã®å®Ÿè¡Œã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆåŠ¹æœé‡ã€ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã€ãƒ‡ãƒ¼ã‚¿åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ã€ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—åˆ—ã€ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚¿ãƒ¼åˆ—ï¼‰ã‚’åé›†ä¸­ã§ã™ã€‚",
                        "collected_params": {
                            "required": collected_params_state.get("required"),
                            "optional": collected_params_state.get("optional")
                        },
                        "missing_required_params": collected_params_state.get("missing_required"),
                        "asked_optional_params": collected_params_state.get("asked_optional"),
                        "last_bot_question": context.get("question_history", {}).get("last_question"),
                        "current_question_target": current_question_target
                    }
                    
                    gemini_response = extract_parameters_from_user_input(
                        user_input=text,
                        data_summary=data_summary,
                        conversation_history=conversation_history_for_gemini,
                        collection_context=collection_context_for_gemini
                    )
                    extracted_params_map = gemini_response.get("extracted_params", {}) if isinstance(gemini_response, dict) else {}
                    if not extracted_params_map:
                        logger.warning(f"Gemini Function Calling failed or returned no params for text '{text}'. Response: {gemini_response}")
                else:
                    logger.info("Text for parameter extraction was empty, skipping Gemini call.")

                parsed_model_type = None
                lower_text = text.lower() 
                if "fixed" in lower_text or "å›ºå®š" in lower_text: parsed_model_type = "fixed"
                elif "random" in lower_text or "ãƒ©ãƒ³ãƒ€ãƒ " in lower_text: parsed_model_type = "random"
                if parsed_model_type and not extracted_params_map.get("model_type"): extracted_params_map["model_type"] = parsed_model_type
                
                is_ready, next_question = self._update_collected_params_and_get_next_question(extracted_params_map, collected_params_state, data_summary, thread_ts, channel_id)
                
                dialog_state["is_initial_response"] = False
                context["dialog_state"]["collected_params"] = collected_params_state

                if is_ready:
                    final_preferences = {
                        "measure": collected_params_state["required"].get("effect_size"),
                        "model_type": collected_params_state["required"].get("model_type"),
                        "subgroup_columns": collected_params_state["optional"].get("subgroup_columns", self.OPTIONAL_PARAMS_DEFINITION.get("subgroup_columns", [])),
                        "moderator_columns": collected_params_state["optional"].get("moderator_columns", self.OPTIONAL_PARAMS_DEFINITION.get("moderator_columns", [])),
                        "sensitivity_variable": collected_params_state["optional"].get("sensitivity_variable"),
                        "sensitivity_value": collected_params_state["optional"].get("sensitivity_value"),
                        "data_columns": collected_params_state["optional"].get("data_columns", {}),
                        "is_log_transformed": collected_params_state["optional"].get("is_log_transformed"),
                        "data_format": collected_params_state["optional"].get("data_format"),
                        "ai_interpretation": True, "output_format": "detailed",
                    }
                    final_preferences["analysis_type"] = self.EFFECT_SIZE_TO_ANALYSIS_TYPE_MAP.get(final_preferences.get("measure"), "meta_analysis_basic")
                    
                    context["analysis_state"] = {"preferences": final_preferences, "stage": "running"}
                    job_id = self.async_runner.run_analysis_async( 
                        run_meta_analysis_func, 
                        {"csv_path": data_state.get("file_path"), "analysis_preferences": final_preferences, "thread_dir": data_state.get("thread_dir")},
                        None
                    )
                    context["analysis_job_id"] = job_id
                    dialog_state["state"] = "analysis_running"
                    
                    param_summary_list = [f"{k}: {v}" for k, v in final_preferences.items() if v and k in ["measure", "model_type", "subgroup_columns", "moderator_columns", "sensitivity_variable", "sensitivity_value"]]
                    confirmation_msg = f"ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ä»¥ä¸‹ã®è¨­å®šã§ãƒ¡ã‚¿ã‚¢ãƒŠãƒªã‚·ã‚¹ã‚’é–‹å§‹ã—ã¾ã™ï¼š\n- {', '.join(param_summary_list)}"
                    client.chat_postMessage(channel=channel_id, thread_ts=thread_ts, text=confirmation_msg)
                    check_analysis_job_func(job_id, channel_id, thread_ts, client) 
                else:
                    if next_question == context["question_history"]["last_question"]: context["question_history"]["count"] += 1
                    else: context["question_history"] = {"last_question": next_question, "count": 1, "max_retries": 5}
                    
                    if context["question_history"]["count"] >= context["question_history"]["max_retries"]:
                        client.chat_postMessage(channel=channel_id, thread_ts=thread_ts, text=f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åé›†ã§å•é¡ŒãŒç™ºç”Ÿã—ãŸã‚ˆã†ã§ã™ï¼ˆåŒã˜è³ªå•ã€Œ{next_question}ã€ãŒ{context['question_history']['max_retries']}å›ç¹°ã‚Šè¿”ã•ã‚Œã¾ã—ãŸï¼‰ã€‚æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚")
                        DialogStateManager.set_dialog_state(context, "WAITING_FILE")
                        context.pop("question_history", None)
                        if collected_params_state:
                            collected_params_state.update({"required": {}, "optional": {}, "missing_required": list(self.REQUIRED_PARAMS_DEFINITION.keys()), "asked_optional": []})
                    else:
                        response = client.chat_postMessage(channel=channel_id, thread_ts=thread_ts, text=next_question)
                        # Botã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡æ™‚åˆ»ã‚’è¨˜éŒ² (parameter_collectorå†…ã§ã‚‚æ›´æ–°)
                        if response.get("ok"):
                            context["last_bot_message"] = {
                                "ts": response.get("ts"),
                                "timestamp": time.time(), # UNIXã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                                "content": next_question
                            }
                            logger.info(f"Updated last_bot_message in parameter_collector: ts={response.get('ts')}")
                        else:
                            logger.error(f"Failed to send message in parameter_collector, not updating last_bot_message. Response: {response}")
            
            except Exception as e:
                error_message = f"Error in handle_analysis_preference_dialog: {type(e).__name__} - {str(e)}"
                logger.error(error_message, exc_info=True) # Keep exc_info for full trace if possible
                logger.info(f"PARAMETER_COLLECTOR_ERROR_DETAIL: {error_message}") # Add specific INFO log for easier finding
                client.chat_postMessage(channel=channel_id, thread_ts=thread_ts, text="ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è©³ç´°ã¯ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            finally:
                context["parameter_collection_in_progress"] = False # å‡¦ç†å®Œäº†æ™‚ã«ãƒ•ãƒ©ã‚°ã‚’ã‚¯ãƒªã‚¢
                if thinking_message_ts:
                    try: client.chat_delete(channel=channel_id, ts=thinking_message_ts)
                    except Exception as e_del: logger.error(f"Failed to delete 'thinking' message: {e_del}")
        else:
            logger.warning(f"Unexpected dialog state or missing collected_params: {dialog_state}")
            client.chat_postMessage(channel=channel_id, thread_ts=thread_ts, text="åˆ†æè¨­å®šã®ç¾åœ¨ã®çŠ¶æ…‹ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            DialogStateManager.set_dialog_state(context, "WAITING_FILE")

        self.context_manager.save_context(thread_ts, context, channel_id)
