import asyncio
import json # è¿½åŠ 
from slack_bolt import App
from core.metadata_manager import MetadataManager
from core.r_executor import RAnalysisExecutor # ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤
from utils.slack_utils import create_analysis_result_message, upload_files_to_slack
from utils.file_utils import get_r_output_dir, cleanup_temp_dir_async, save_content_to_temp_file # file_utils ã‹ã‚‰é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# upload_files_to_slack ã¯ utils.slack_utils ã«ä½œæˆã™ã‚‹ãŒã€ã“ã“ã§ã¯ä¸€æ—¦ãƒ€ãƒŸãƒ¼ã‚’å®šç¾©ã—ã¦ãŠã
# async def upload_files_to_slack(files_to_upload: list, channel_id: str, thread_ts: str, client, job_id: str):
#     print(f"ä»®ã®upload_files_to_slackãŒå‘¼ã³å‡ºã•ã‚Œã¾ã—ãŸã€‚job_id: {job_id}, files: {files_to_upload}")
#     await asyncio.sleep(1)
#     return [{"type": f["type"], "id": f"F_UPLOADED_{f['type'].upper()}", "path": f["path"]} for f in files_to_upload]


def register_analysis_handlers(app: App):
    """è§£æé–¢é€£ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ç™»éŒ²"""
    
    @app.action("start_analysis") # è¨ˆç”»æ›¸ã§ã¯ start_analysis ã ãŒã€csv_handler ã‹ã‚‰ã®é€£æºã‚’è€ƒãˆã‚‹ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ã‚Š/ãªã—ã§åˆ†ã‘ã‚‹ã¹ã
                                 # ã“ã“ã§ã¯ä¸€æ—¦ start_analysis ã®ã¾ã¾é€²ã‚ã€å¾Œã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åé›†ãƒ•ãƒ­ãƒ¼ã¨åˆã‚ã›ã¦èª¿æ•´ã™ã‚‹
    def handle_analysis_start(ack, body, client, logger):
        """è§£æé–‹å§‹ãƒœã‚¿ãƒ³ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
        ack()
        
        payload = MetadataManager.extract_from_body(body)
        
        if not payload:
            client.chat_postMessage(
                channel=body["channel"]["id"],
                text="âŒ è§£ææƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
            )
            return
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’payloadã‹ã‚‰å–å¾—ã™ã‚‹æƒ³å®š (ãƒ¢ãƒ¼ãƒ€ãƒ«ç­‰ã§è¨­å®šã•ã‚ŒãŸå ´åˆ)
        # ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        user_parameters = payload.get("user_parameters", {
            "measure": payload.get("csv_analysis", {}).get("suggested_analysis", {}).get("effect_type", "SMD"),
            "model": payload.get("csv_analysis", {}).get("suggested_analysis", {}).get("model_type", "random")
        })
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
        job_id = payload.get('job_id', 'unknown_job')
        r_output_dir = get_r_output_dir(job_id)

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚‚RãŒã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ä¸€æ™‚ä¿å­˜ã™ã‚‹ (file_urlã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜)
        # csv_handlerã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’metadataçµŒç”±ã§æ¸¡ã™ã‹ã€ã“ã“ã§å†åº¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹æ¤œè¨
        # ã“ã“ã§ã¯ã€payloadã« file_url ãŒã‚ã‚‹å‰æã§å†åº¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜ã™ã‚‹
        # (ã‚ˆã‚ŠåŠ¹ç‡çš„ãªã®ã¯csv_handlerã§ä¿å­˜ã—ãŸãƒ‘ã‚¹ã‚’metadataã§æ¸¡ã™ã“ã¨)
        
        # payloadã‹ã‚‰å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«IDã¨URLã‚’å–å¾—
        original_file_id = payload.get("file_id")
        original_file_url = payload.get("file_url") # csv_handlerã§ä¿å­˜ã—ãŸURL
        original_file_name = payload.get("csv_analysis", {}).get("original_filename", "data.csv") # Geminiåˆ†æçµæœã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åå–å¾—

        asyncio.create_task(run_analysis_async(
            payload=payload,
            user_parameters=user_parameters,
            channel_id=body["channel"]["id"],
            thread_ts=body["message"]["ts"],
            user_id=body["user"]["id"],
            client=client,
            logger=logger,
            r_output_dir=r_output_dir, # Rã®å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            original_file_url=original_file_url, # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®URL
            original_file_name=original_file_name # CSVãƒ•ã‚¡ã‚¤ãƒ«å
        ))
        
        client.chat_postMessage(
            channel=body["channel"]["id"],
            thread_ts=body["message"]["ts"],
            text="ğŸ”„ è§£æã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚å®Œäº†ã¾ã§å°‘ã€…ãŠå¾…ã¡ãã ã•ã„..."
        )

async def run_analysis_async(payload, user_parameters, channel_id, thread_ts, user_id, client, logger, r_output_dir, original_file_url, original_file_name):
    """ãƒ¡ã‚¿è§£æã®éåŒæœŸå®Ÿè¡Œ"""
    temp_csv_path = None
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¸€æ™‚ä¿å­˜
        if original_file_url:
            from utils.file_utils import download_slack_file_content_async # ã“ã“ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            csv_bytes = await download_slack_file_content_async(original_file_url, client.token)
            temp_csv_path_str, temp_csv_path_obj = await save_content_to_temp_file(
                csv_bytes, payload["job_id"], original_filename=original_file_name
            )
            temp_csv_path = temp_csv_path_obj # Pathã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å¾Œã§ä½¿ã†
        else:
            logger.error("run_analysis_async: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®URLãŒpayloadã«ã‚ã‚Šã¾ã›ã‚“ã€‚")
            raise ValueError("CSV file URL is missing.")

        r_executor = RAnalysisExecutor(r_output_dir=r_output_dir, csv_file_path=temp_csv_path, job_id=payload["job_id"])
        
        # data_summary ã‚’æº–å‚™ï¼ˆCSVã®åŸºæœ¬æƒ…å ±ï¼‰
        csv_analysis = payload.get("csv_analysis", {})
        
        # CSVã®åˆ—æƒ…å ±ã‚’å–å¾—ï¼ˆGeminiã®åˆ†æçµæœã‹ã‚‰ï¼‰
        column_descriptions = csv_analysis.get("column_descriptions", {})
        csv_columns = list(column_descriptions.keys()) if column_descriptions else []
        
        # data_previewã‹ã‚‰ã‚‚åˆ—åã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        data_preview = csv_analysis.get("data_preview", [])
        if not csv_columns and data_preview:
            csv_columns = list(data_preview[0].keys()) if data_preview else []
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 
        logger.info(f"Debug - CSV column extraction: column_descriptions keys: {list(column_descriptions.keys()) if column_descriptions else 'None'}")
        logger.info(f"Debug - CSV column extraction: data_preview sample: {data_preview[0] if data_preview else 'None'}")
        logger.info(f"Debug - CSV column extraction: final csv_columns: {csv_columns}")
        
        data_summary = {
            "csv_file_path": str(temp_csv_path),
            "csv_analysis": csv_analysis,
            "detected_columns": csv_analysis.get("detected_columns", {}),
            "columns": csv_columns,  # åˆ—æƒ…å ±ã‚’è¿½åŠ 
            "file_info": {
                "filename": original_file_name,
                "job_id": payload["job_id"]
            }
        }
        
        analysis_result_from_r = await r_executor.execute_meta_analysis(
            analysis_params=user_parameters,
            data_summary=data_summary
        )
        
        # analysis_result_from_r["files"] ã¯ {"type": "path"} ã®è¾æ›¸ã‚’æƒ³å®š
        files_to_upload_for_slack = []
        if analysis_result_from_r.get("success") and analysis_result_from_r.get("generated_plots_paths"):
            for plot_info in analysis_result_from_r.get("generated_plots_paths", []): # RExecutorã‹ã‚‰ã®æˆ»ã‚Šå€¤ã«åˆã‚ã›ã‚‹
                 files_to_upload_for_slack.append({"type": plot_info["label"], "path": plot_info["path"], "title": plot_info["label"]})
            if analysis_result_from_r.get("structured_summary_json_path"):
                files_to_upload_for_slack.append({
                    "type": "summary_json", 
                    "path": analysis_result_from_r["structured_summary_json_path"],
                    "title": f"summary_{payload['job_id']}.json"
                })
            if analysis_result_from_r.get("rdata_path"):
                 files_to_upload_for_slack.append({
                    "type": "rdata",
                    "path": analysis_result_from_r["rdata_path"],
                    "title": f"result_{payload['job_id']}.RData"
                })
            if analysis_result_from_r.get("r_script_path"):
                 files_to_upload_for_slack.append({
                    "type": "r_script",
                    "path": analysis_result_from_r["r_script_path"],
                    "title": f"run_meta_{payload['job_id']}.R"
                })


        files_uploaded_info = await upload_files_to_slack( # å®Ÿéš›ã®é–¢æ•°å‘¼ã³å‡ºã—
            files_to_upload=files_to_upload_for_slack,
            channel_id=channel_id,
            thread_ts=thread_ts,
            client=client,
            job_id=payload["job_id"]
        )
        
        # Rã®å®Ÿè¡Œçµæœã‹ã‚‰ã‚µãƒãƒªãƒ¼ã‚’å–å¾— (structured_summary_content ã‚’ä½¿ã†)
        r_summary_for_metadata = {}
        if analysis_result_from_r.get("success") and analysis_result_from_r.get("structured_summary_content"):
            try:
                full_r_summary = json.loads(analysis_result_from_r["structured_summary_content"])
                
                # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’å«ã‚€å®Œå…¨ãªã‚µãƒãƒªãƒ¼ã‚’ä¿æŒã—ã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã§ä½¿ç”¨ã™ã‚‹
                r_summary_for_metadata = full_r_summary.copy()
                
                # overall_analysisãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ãã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã«è¿½åŠ ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
                # ãŸã ã—ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã¯ä¸Šæ›¸ãã—ãªã„
                if "overall_analysis" in full_r_summary:
                    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’ä¿æŒ
                    version_info = {
                        'r_version': r_summary_for_metadata.get('r_version'),
                        'metafor_version': r_summary_for_metadata.get('metafor_version'),
                        'analysis_environment': r_summary_for_metadata.get('analysis_environment')
                    }
                    
                    r_summary_for_metadata.update(full_r_summary["overall_analysis"])
                    
                    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’å¾©å…ƒ
                    for key, value in version_info.items():
                        if value is not None:
                            r_summary_for_metadata[key] = value
                    
            except json.JSONDecodeError:
                logger.error("Rã‹ã‚‰ã®JSONã‚µãƒãƒªãƒ¼ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                r_summary_for_metadata = {"error": "Failed to parse R summary JSON"}


        completion_metadata = MetadataManager.create_metadata("analysis_complete", {
            "job_id": payload["job_id"],
            "result_summary": r_summary_for_metadata, # Rã®ã‚µãƒãƒªãƒ¼ã‚’ä½¿ç”¨
            "uploaded_files": files_uploaded_info,
            "r_stdout": analysis_result_from_r.get("stdout", ""),
            "r_stderr": analysis_result_from_r.get("stderr", ""),
            "r_script_path": analysis_result_from_r.get("r_script_path", ""), # å‚è€ƒç”¨
            "stage": "awaiting_interpretation",
            "user_id": user_id,
            "original_file_id": payload.get("file_id"),
            "original_file_url": payload.get("file_url") # ã“ã‚Œã¯ run_analysis_async ã«æ¸¡ã•ã‚ŒãŸã‚‚ã®
        })
        
        # create_analysis_result_blocks ã«æ¸¡ã™ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’Rã®å‡ºåŠ›ã«åˆã‚ã›ã‚‹
        # RExecutorã®æˆ»ã‚Šå€¤ã® "summary" ãŒ create_analysis_result_blocks ã®æœŸå¾…ã™ã‚‹æ§‹é€ ã¨ç•°ãªã‚‹å ´åˆã€ã“ã“ã§å¤‰æ›ã™ã‚‹
        # ã“ã“ã§ã¯ã€r_summary_for_metadata ã‚’ãã®ã¾ã¾ä½¿ãˆã‚‹ã‚ˆã†ã« create_analysis_result_blocks å´ã‚’èª¿æ•´ã™ã‚‹ã‹ã€
        # ã“ã“ã§æœŸå¾…ã•ã‚Œã‚‹æ§‹é€ ã«æ•´å½¢ã™ã‚‹ã€‚
        # ä»Šå›ã¯ r_summary_for_metadata ã‚’ãã®ã¾ã¾æ¸¡ã—ã€create_analysis_result_blocks ã§å¯¾å¿œã™ã‚‹ã¨ä»®å®šã€‚
        # ãŸã ã—ã€ãƒ­ã‚°è¡¨ç¤ºã®ãŸã‚ã« analysis_result_from_r å…¨ä½“ã‚‚æ¸¡ã™ã€‚
        display_result_for_blocks = {
            "summary": r_summary_for_metadata,
            "r_log": analysis_result_from_r.get("stdout","") + "\n" + analysis_result_from_r.get("stderr","")
        }

        result_message = create_analysis_result_message(display_result_for_blocks)
        client.chat_postMessage(
            channel=channel_id,
            thread_ts=thread_ts,
            text=result_message,
            metadata=completion_metadata
        )
        
        # è§£é‡ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’è‡ªå‹•çš„ã«é–‹å§‹
        from handlers.report_handler import generate_report_async
        
        # report_handlerãŒæœŸå¾…ã™ã‚‹payloadæ§‹é€ ã‚’ä½œæˆ
        report_payload = {
            "job_id": payload["job_id"],
            "result_summary": r_summary_for_metadata,
            "stage": "awaiting_interpretation",
            "user_id": user_id,
            "original_file_id": payload.get("file_id"),
            "uploaded_files": files_uploaded_info,
            "r_stdout": analysis_result_from_r.get("stdout", ""),
            "r_stderr": analysis_result_from_r.get("stderr", "")
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
        client.chat_postMessage(
            channel=channel_id,
            thread_ts=thread_ts,
            text="ğŸ“ è§£é‡ˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­ã§ã™..."
        )
        
        # åŒæœŸçš„ã«ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œ
        await generate_report_async(
            payload=report_payload,
            channel_id=channel_id,
            thread_ts=thread_ts,
            client=client,
            logger=logger
        )
        
    except Exception as e:
        logger.error(f"è§£æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        client.chat_postMessage(
            channel=channel_id,
            thread_ts=thread_ts,
            text=f"âŒ è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )
    finally:
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if temp_csv_path and temp_csv_path.parent.exists(): # CSVã‚’ä¿å­˜ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            await cleanup_temp_dir_async(temp_csv_path.parent)
        if r_output_dir.exists(): # Rã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            await cleanup_temp_dir_async(r_output_dir)
        logger.info(f"è§£æå®Œäº†å¾Œã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è©¦è¡Œå®Œäº†ã€‚")
